{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/creditcardfraud/creditcard.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['Class']\n",
    "X = df.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3\n",
    "    >SMOTETmek -- This is a method which basically combine and use both over sampling and under sampling technique for converting the imbalanced data to balanced data</h3>\n",
    "\n",
    "SMOTETomek method is efficient under a small dataset, and the accuracy of personality recognition is improved by up to around 10%. The results are better than those of previous similar studies. The average accuracies of the plain text dataset and the non-plain text dataset are 75.34% and 78.78%, respectively. The average accuracies of the short text dataset and the long text dataset are 75.34% and 64.25%, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTETomek(sampling_strategy = 'minority',random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the fit_sample method to convert the imbalanced data into balanced data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new,Y_new = smote.fit_sample(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Printing the balanced data shape which is X_new,Y_new <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_new.shape,Y_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the data by using the shuffle method which is present in sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new,Y_new = shuffle(X_new,Y_new,random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking the fraud and correct card data samples and storing them into different lists. <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_card = X_new[Y_new == 1]\n",
    "correct_card = X_new[Y_new == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the shape of fraud and correct card data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fraud_card.shape,correct_card.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Doing the Visualisation by using count plot to know count of fraud and non fraud samples present in imbalanced data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Doing the Visualisation by using count plot to know count of fraud and non fraud samples present in imbalanced data</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(Y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the correlation matrix of imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))  \n",
    "sns.heatmap(X.corr(),cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the correlation matrix of balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))  \n",
    "# sns.set_palette('coolwarm')\n",
    "sns.heatmap(X_new.corr(),cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the balanced data into training and testing set by using train_test_split method from sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X_new,Y_new,test_size=0.3,random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the shapes of our new training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><h1>Modelling</h1></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now Let's Start Building Credit Card Fraud Detection model using Logistic regression<h2>\n",
    "    \n",
    "<p>Importing the Logistic Regression from sklearn.linear_models and then create the object of it namely model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Just Fit Your Model with training data by using fit method which takes imput as X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Make Prediction by passing a testing data to the predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Calculate and Print the Accuracy Score By passing actual label with the predict label to accuracy_score function that you can import from sklearn.metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(Y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now We are going to plot confusion_matrix and classificaion report to check our model performance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,pred))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(confusion_matrix(Y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>As We Will See We Have Got A Much Better Result But We Are Trying To Improve More By Testing Out Various Classification models on it to improve our accuracy</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier,SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Now we are going to make a function whose parameters are traing and testing data</h3>\n",
    "<h3>First We Just form the list of tuples in which the tuples consist of two elements first one is Current Classifier Object aand Second is the Model Name</h3>\n",
    "<h3>Then after that we are going devlop an list of tuples in which first element of tuple represents accuracy of the model , Second element of tuple represents classifier or model name, Third element represents mean_squared_error encountered in the current model </h3>\n",
    "<h3>Then we just iterate through models list one by one and store the results in the accuracy_list and at the end return the accuracy_list</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_various_classifiers(X_train,Y_train,X_test,Y_test):\n",
    "    models = [  \n",
    "            \n",
    "            (ExtraTreesClassifier(),'ExtraTreesClassifier'),\n",
    "            (GradientBoostingClassifier(),'GradientBoostingClassifier'),\n",
    "            (RandomForestClassifier(),'RandomForestClassifier'),\n",
    "            (RidgeClassifier(),'RidgeClassifier'),\n",
    "            (SGDClassifier(),'SGDClassifier'),\n",
    "            (LogisticRegression(),'LogisticRegression'),\n",
    "            (AdaBoostClassifier(),'Ada Boost Classifier'),\n",
    "            (xgb.XGBClassifier(),'XGBClassifier') \n",
    "    ]\n",
    "    accuracy_list = []\n",
    "    for (model,model_name) in models:\n",
    "        curr_model = model\n",
    "        curr_model.fit(X_train,Y_train)\n",
    "        pred = curr_model.predict(X_test)\n",
    "        acc = accuracy_score(Y_test,pred)\n",
    "        err = mean_squared_error(Y_test,pred)\n",
    "        accuracy_list.append((acc,model_name,err))\n",
    "        print(str(acc) + ' '+ str(err))\n",
    "    return accuracy_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now we are going to use our check_various_classifiers function by giving parameters as (X_train,Y_train) -> Training Data , (X_test,Y_test) -> Testing Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = check_various_classifiers(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now we are sorting our returned list on the basis of accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list.sort(reverse=True,key = lambda x:x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Now we are going to display the final result in which we are printing the model_name along with it's accuracy and error.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "accuracies = []\n",
    "error = []\n",
    "for (acc,model_name,err) in accuracy_list:\n",
    "    \n",
    "    model_names.append(model_name)\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    error.append(err)\n",
    "\n",
    "    \n",
    "accuracy_list.sort(reverse=True,key = lambda x:x[0])\n",
    "\n",
    "for (acc,model,loss) in accuracy_list:\n",
    "    print('Accuracy of '+ str(model)+ ' '+str(acc)+' with loss of '+str(loss))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now We are going to display the accuracies along with its models using barplots and doing the visualization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.barplot(model_names,accuracies)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now We are going to display the mean_squared_error along with its models using barplots in seaborn</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.barplot(model_names,error)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>By Doing The Visualization we can see that best classifier which gives high accuracy is ExtraTreesClassifier with the accuracy of 99.986% and with the loss of 0.0135%<h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
